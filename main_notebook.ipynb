{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfa8f6058f07c39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Human Value Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47fe4c58990df29",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9333223475180",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import transformers\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utilities import *\n",
    "from models.bertOne import BertOne\n",
    "from models.berTwo import BerTwo\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n",
    "from models.randomUniformClassifier import RandomUniformClassifier\n",
    "from models.majorityCalssifier import MajorityClassifier\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from drTorch.callbacks import EarlyStopper\n",
    "\n",
    "from drTorch.metrics import F1_Score\n",
    "from drTorch.metrics import F1_Score_Multi_Labels\n",
    "from drTorch.utilities import *\n",
    "from drTorch.wrappers import OptimizerWrapper\n",
    "from drTorch.wrappers import Criterion\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dcff5a5f7bfb48",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print('Device: %s' % device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2553b6aeb5c7f9",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef2ee4b05f9169d2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Defining constants and flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PATHS AND DATAFRAME CREATION\n",
    "DATA_DIR = \"data\"\n",
    "ARGUMENTS_DIR = os.path.join(DATA_DIR, \"arguments\")\n",
    "LABELS_DIR = os.path.join(DATA_DIR, \"labels\")\n",
    "\n",
    "BERT_MODELS_DIRECTORY = \"bert_models\"\n",
    "BERT_VERSIONS=[\"bert-base-uncased\"]\n",
    "\n",
    "# CONSTANTS \n",
    "N_LABELS = 4\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 1\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "CLASS_2_ONE_HOT = {class_label: np.eye(N_CLASSES)[i].astype(float).tolist() for i, class_label in enumerate(range(N_CLASSES))}\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7d71adb601979737"
  },
  {
   "cell_type": "markdown",
   "id": "32a409b1e982e3dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723181a1d5f1aad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910abdb6bbf8ee01",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# convert files in dataframes\n",
    "train_arg_df, val_arg_df, test_arg_df = create_dfs(ARGUMENTS_DIR)\n",
    "train_labels_df, val_labels_df, test_labels_df = create_dfs(LABELS_DIR)\n",
    "\n",
    "print(\"Let's visualize the data: \")\n",
    "display(train_arg_df.head(5))\n",
    "display(train_labels_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8c631fc728aa8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Mapping labels to level-3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d8c9bd76a90f4",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mapping = define_mapping()\n",
    "train_labels_df, val_labels_df, test_labels_df = map_to_level_3(mapping, train_labels_df, val_labels_df, test_labels_df) \n",
    "\n",
    "print(\"The training labels after the mapping are the following: \")\n",
    "train_labels_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3b5162e441c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### One-hot encoding, tokenization and data loaders building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.exists(BERT_MODELS_DIRECTORY):\n",
    "    bert_versions_paths = os.listdir(BERT_MODELS_DIRECTORY)\n",
    "else:    \n",
    "    bert_versions_paths = download_bert_models(BERT_MODELS_DIRECTORY, BERT_VERSIONS)  \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1dad71e1264e68ca"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e75074e5c73c20",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T09:32:03.476338400Z",
     "start_time": "2023-12-14T09:32:02.681877900Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m\"\u001B[39m:[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConclusion\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCP\u001B[39m\u001B[38;5;124m\"\u001B[39m:[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConclusion\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPremise\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCPS\u001B[39m\u001B[38;5;124m\"\u001B[39m:[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConclusion\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPremise\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStance\u001B[39m\u001B[38;5;124m\"\u001B[39m]}\n\u001B[1;32m----> 3\u001B[0m dataloader_train_C, dataloader_val_C, dataloader_test_C  \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_dataloaders\u001B[49m(train_df\u001B[38;5;241m=\u001B[39mtrain_arg_df, \n\u001B[0;32m      4\u001B[0m                                                                              val_df\u001B[38;5;241m=\u001B[39mval_arg_df, \n\u001B[0;32m      5\u001B[0m                                                                              test_df\u001B[38;5;241m=\u001B[39mtest_arg_df, \n\u001B[0;32m      6\u001B[0m                                                                              train_labels_df\u001B[38;5;241m=\u001B[39mtrain_labels_df, \n\u001B[0;32m      7\u001B[0m                                                                              val_labels_df\u001B[38;5;241m=\u001B[39mval_labels_df, \n\u001B[0;32m      8\u001B[0m                                                                              test_labels_df\u001B[38;5;241m=\u001B[39mtest_labels_df, \n\u001B[0;32m      9\u001B[0m                                                                              one_hot_mapping\u001B[38;5;241m=\u001B[39mCLASS_2_ONE_HOT, \n\u001B[0;32m     10\u001B[0m                                                                              bert_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n\u001B[0;32m     11\u001B[0m                                                                              model_input\u001B[38;5;241m=\u001B[39mmodel_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     12\u001B[0m                                                                              custom_dataset_builder\u001B[38;5;241m=\u001B[39mCustomDataset_C,\n\u001B[0;32m     13\u001B[0m                                                                              batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, \n\u001B[0;32m     14\u001B[0m                                                                              shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     16\u001B[0m dataloader_train_CP, dataloader_val_CP, dataloader_test_CP  \u001B[38;5;241m=\u001B[39m build_dataloaders(train_df\u001B[38;5;241m=\u001B[39mtrain_arg_df, \n\u001B[0;32m     17\u001B[0m                                                                                 val_df\u001B[38;5;241m=\u001B[39mval_arg_df, \n\u001B[0;32m     18\u001B[0m                                                                                 test_df\u001B[38;5;241m=\u001B[39mtest_arg_df, \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m                                                                                 batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, \n\u001B[0;32m     27\u001B[0m                                                                                 shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     29\u001B[0m dataloader_train_CPS, dataloader_val_CPS, dataloader_test_CPS  \u001B[38;5;241m=\u001B[39m build_dataloaders(train_df\u001B[38;5;241m=\u001B[39mtrain_arg_df, \n\u001B[0;32m     30\u001B[0m                                                                                    val_df\u001B[38;5;241m=\u001B[39mval_arg_df, \n\u001B[0;32m     31\u001B[0m                                                                                    test_df\u001B[38;5;241m=\u001B[39mtest_arg_df, \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     39\u001B[0m                                                                                    batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, \n\u001B[0;32m     40\u001B[0m                                                                                    shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'build_dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "model_inputs = {\"C\":[\"Conclusion\"], \"CP\":[\"Conclusion\", \"Premise\"], \"CPS\":[\"Conclusion\", \"Premise\", \"Stance\"]}\n",
    "\n",
    "dataloader_train_C, dataloader_val_C, dataloader_test_C  = build_dataloaders(train_df=train_arg_df, \n",
    "                                                                             val_df=val_arg_df, \n",
    "                                                                             test_df=test_arg_df, \n",
    "                                                                             train_labels_df=train_labels_df, \n",
    "                                                                             val_labels_df=val_labels_df, \n",
    "                                                                             test_labels_df=test_labels_df, \n",
    "                                                                             one_hot_mapping=CLASS_2_ONE_HOT, \n",
    "                                                                             bert_version=\"bert-base-uncased\", \n",
    "                                                                             model_input=model_inputs[\"C\"],\n",
    "                                                                             custom_dataset_builder=CustomDataset_C,\n",
    "                                                                             batch_size=BATCH_SIZE, \n",
    "                                                                             shuffle=True)\n",
    "\n",
    "dataloader_train_CP, dataloader_val_CP, dataloader_test_CP  = build_dataloaders(train_df=train_arg_df, \n",
    "                                                                                val_df=val_arg_df, \n",
    "                                                                                test_df=test_arg_df, \n",
    "                                                                                train_labels_df=train_labels_df, \n",
    "                                                                                val_labels_df=val_labels_df, \n",
    "                                                                                test_labels_df=test_labels_df, \n",
    "                                                                                one_hot_mapping=CLASS_2_ONE_HOT, \n",
    "                                                                                bert_version=\"bert-base-uncased\", \n",
    "                                                                                model_input=model_inputs[\"CP\"],\n",
    "                                                                                custom_dataset_builder=CustomDataset_CP,\n",
    "                                                                                batch_size=BATCH_SIZE, \n",
    "                                                                                shuffle=True)\n",
    "\n",
    "dataloader_train_CPS, dataloader_val_CPS, dataloader_test_CPS  = build_dataloaders(train_df=train_arg_df, \n",
    "                                                                                   val_df=val_arg_df, \n",
    "                                                                                   test_df=test_arg_df, \n",
    "                                                                                   train_labels_df=train_labels_df, \n",
    "                                                                                   val_labels_df=val_labels_df, \n",
    "                                                                                   test_labels_df=test_labels_df, \n",
    "                                                                                   one_hot_mapping=CLASS_2_ONE_HOT, \n",
    "                                                                                   bert_version=\"bert-base-uncased\", \n",
    "                                                                                   model_input=model_inputs[\"CPS\"],\n",
    "                                                                                   custom_dataset_builder=CustomDataset_CPS,\n",
    "                                                                                   batch_size=BATCH_SIZE, \n",
    "                                                                                   shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b4d8eb2b9080d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:45:19.405346392Z",
     "start_time": "2023-12-13T16:45:13.535600867Z"
    }
   },
   "source": [
    "## Task 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbdf03bd612820",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Models Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3848b4411be166",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:45:34.405111405Z",
     "start_time": "2023-12-13T16:45:34.363675874Z"
    }
   },
   "source": [
    "####  1) Random uniform classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f51d9c86bef3a0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T09:32:03.500343600Z",
     "start_time": "2023-12-14T09:32:03.483341400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of the random uniform classifier\n",
    "random_classifier = RandomUniformClassifier(N_LABELS)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_labels = random_classifier.predict(test_arg_df)\n",
    "\n",
    "# Accuracy of the Random Classifier\n",
    "accuracy = accuracy_score(test_labels_df, predicted_labels)\n",
    "print(f'Accuracy of the model over all the classes: {accuracy}\\n')\n",
    "\n",
    "# F1 scores on the different labels singularly taken \n",
    "f1_scorer = F1_Score_Multi_Labels(name='F1_macro_avg', num_labels=N_LABELS, num_classes=N_CLASSES, compute_mean=False)\n",
    "f1_scores_random_classifier = f1_scorer(torch.tensor(test_labels_df.values.tolist()), torch.tensor(predicted_labels))\n",
    "# AVG f1 score\n",
    "f1_avg_random_classifier = np.average(f1_scores_random_classifier)\n",
    "\n",
    "\n",
    "for idx, f1_score in enumerate(f1_scores_random_classifier):\n",
    "    print(f\" - {train_labels_df.columns[idx]} F1: {f1_score}\")\n",
    "\n",
    "print(f'\\nAverage F1: {f1_avg_random_classifier}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db911208037aa18",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  1) Majority classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "majority_classifier = MajorityClassifier()\n",
    "\n",
    "# Train the majority classifier (even though in practice, no training is needed)\n",
    "majority_classifier.fit(train_labels_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_labels = majority_classifier.predict(test_labels_df)\n",
    "\n",
    "# Accuracy of the Majority Classifier\n",
    "accuracy = accuracy_score(test_labels_df, predicted_labels)\n",
    "print(f'Accuracy of the model over all the classes: {accuracy}\\n')\n",
    "\n",
    "# F1 scores on the different labels singularly taken \n",
    "f1_scorer = F1_Score_Multi_Labels(name='F1_macro_avg', num_labels=N_LABELS, num_classes=N_CLASSES, compute_mean=False)\n",
    "f1_scores_majority_classifier = f1_scorer(torch.tensor(test_labels_df.values.tolist()), torch.tensor(predicted_labels))\n",
    "# AVG F1 score\n",
    "f1_avg_majority_classifier = np.average(f1_scores_majority_classifier)\n",
    "\n",
    "\n",
    "for idx, f1_score in enumerate(f1_scores_majority_classifier):\n",
    "    print(f\" - {train_labels_df.columns[idx]} F1: {f1_score}\")\n",
    "\n",
    "print(f'\\nAverage F1: {f1_avg_majority_classifier}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T09:32:03.487341100Z"
    }
   },
   "id": "7948530f7e667126"
  },
  {
   "cell_type": "markdown",
   "id": "ed615adcd88b156c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  3) Bert w/C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185abecdef2d8b0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-14T09:32:03.492340200Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_test = OptimizerWrapper(torch.optim.Adam, identifier=f'lr={1e-5}', optimizer_partial_params={'lr': 1e-6})          \n",
    "criterion_test = Criterion('loss', loss_function=torch.nn.BCEWithLogitsLoss(reduction='none'), reduction_function=torch.sum)\n",
    "\n",
    "bert1 = BertOne(dropout_prob=0.3, hidden_size= 768, bert_version='bert-base-uncased').to(device)\n",
    "\n",
    "bert1_history = bert1.fit(train_loader=dataloader_train_C, \n",
    "                          val_loader=dataloader_val_C, \n",
    "                          criterion=criterion_test, \n",
    "                          metrics=[F1_Score_Multi_Labels('F1_macro_avg', num_labels=N_LABELS, num_classes=N_CLASSES)], \n",
    "                          optimizer=optimizer_test,\n",
    "                          # early_stopper=EarlyStopper(monitor='F1_macro', patience=4, delta=0, mode='max', restore_weights=True),\n",
    "                          num_epochs=200)\n",
    "\n",
    "\"\"\"\n",
    "loss homework 1 con batch size 1\n",
    "[0.4, 0.5, 0.8, ... 0.2] 46 loss\n",
    "\n",
    "loss homework 2 con batch size 1\n",
    "        [[0.4339, 0.4490],\n",
    "        [0.5538, 0.6676],\n",
    "        [0.4004, 0.3562],\n",
    "        [0.6964, 0.6620]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4) Bert w/CP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "813fade858d92531"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer_test = OptimizerWrapper(torch.optim.Adam, identifier=f'lr={1e-6}', optimizer_partial_params={'lr': 1e-6})          \n",
    "criterion_test = Criterion('loss', loss_function=torch.nn.BCEWithLogitsLoss(reduction='none'), reduction_function=torch.mean)\n",
    "\n",
    "bert2 = BerTwo(dropout_prob=0.3, hidden_size= 768, bert_version='bert-base-uncased').to(device)\n",
    "\n",
    "bert2_history = bert2.fit(train_loader=dataloader_train_CP, \n",
    "                          val_loader=dataloader_val_CP, \n",
    "                          criterion=criterion_test, \n",
    "                          metrics=[F1_Score_Multi_Labels('F1_macro_avg', num_labels=N_LABELS, num_classes=N_CLASSES)], \n",
    "                          optimizer=optimizer_test,\n",
    "                          # early_stopper=EarlyStopper(monitor='F1_macro', patience=4, delta=0, mode='max', restore_weights=True),\n",
    "                          num_epochs=200)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T09:32:03.497340300Z"
    }
   },
   "id": "a536ac48aad19715"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829599ba89a99187",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T09:32:03.567867600Z",
     "start_time": "2023-12-14T09:32:03.503339600Z"
    }
   },
   "outputs": [],
   "source": [
    "### Test sul modello\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config=transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
    "text = \"Replace me by any text you'd like pollo.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "output = model(**encoded_input)\n",
    "print(model.config)\n",
    "\"\"\"p_d = BertOne()\n",
    "output1 = p_d(**encoded_input)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn.functional import max_pool2d\n",
    "\n",
    "print(output[0].shape)\n",
    "output_2 = max_pool2d(output[0], kernel_size=(14,1))\n",
    "\n",
    "output_2[:,0,:].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T09:32:03.506339900Z"
    }
   },
   "id": "c2bf2f8851186caf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
