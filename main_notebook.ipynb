{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfa8f6058f07c39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Human Value Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47fe4c58990df29",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9333223475180",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:49:32.836775670Z",
     "start_time": "2023-12-13T11:49:31.265703235Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import transformers\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utilities import *\n",
    "from models.bertOne import BertOne\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n",
    "from models.randomUniformClassifier import RandomUniformClassifier\n",
    "from models.majorityCalssifier import MajorityClassifier\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from drTorch.callbacks import EarlyStopper\n",
    "\n",
    "from drTorch.metrics import F1_Score\n",
    "from drTorch.metrics import F1_Score_Multi_Labels\n",
    "from drTorch.utilities import *\n",
    "from drTorch.wrappers import OptimizerWrapper\n",
    "from drTorch.wrappers import Criterion\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29dcff5a5f7bfb48",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:49:32.888803946Z",
     "start_time": "2023-12-13T11:49:32.883037585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print('Device: %s' % device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2553b6aeb5c7f9",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef2ee4b05f9169d2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Defining constants and flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb44f1708144b43a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:49:32.899619416Z",
     "start_time": "2023-12-13T11:49:32.886844953Z"
    }
   },
   "outputs": [],
   "source": [
    "# PATHS AND DATAFRAME CREATION\n",
    "DATA_DIR = \"data\"\n",
    "ARGUMENTS_DIR = os.path.join(DATA_DIR, \"arguments\")\n",
    "LABELS_DIR = os.path.join(DATA_DIR, \"labels\")\n",
    "\n",
    "BERT_MODELS_DIRECTORY = \"bert_models\"\n",
    "BERT_VERSIONS=[\"bert-base-uncased\"]\n",
    "\n",
    "# CONSTANTS \n",
    "N_LABELS = 4\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 1\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "CLASS_2_ONE_HOT = {class_label: np.eye(N_CLASSES)[i].astype(float).tolist() for i, class_label in enumerate(range(N_CLASSES))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a409b1e982e3dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723181a1d5f1aad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910abdb6bbf8ee01",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:49:32.963357957Z",
     "start_time": "2023-12-13T11:49:32.901222333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's visualize the data: \n"
     ]
    },
    {
     "data": {
      "text/plain": "                                              Conclusion       Stance  \\\nArgument ID                                                             \nA01002                       We should ban human cloning  in favor of   \nA01005                           We should ban fast food  in favor of   \nA01006       We should end the use of economic sanctions      against   \nA01007              We should abolish capital punishment      against   \nA01008                     We should ban factory farming      against   \n\n                                                       Premise  \nArgument ID                                                     \nA01002       we should ban human cloning as it will only ca...  \nA01005       fast food should be banned because it is reall...  \nA01006       sometimes economic sanctions are the only thin...  \nA01007       capital punishment is sometimes the only optio...  \nA01008       factory farming allows for the production of c...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Conclusion</th>\n      <th>Stance</th>\n      <th>Premise</th>\n    </tr>\n    <tr>\n      <th>Argument ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A01002</th>\n      <td>We should ban human cloning</td>\n      <td>in favor of</td>\n      <td>we should ban human cloning as it will only ca...</td>\n    </tr>\n    <tr>\n      <th>A01005</th>\n      <td>We should ban fast food</td>\n      <td>in favor of</td>\n      <td>fast food should be banned because it is reall...</td>\n    </tr>\n    <tr>\n      <th>A01006</th>\n      <td>We should end the use of economic sanctions</td>\n      <td>against</td>\n      <td>sometimes economic sanctions are the only thin...</td>\n    </tr>\n    <tr>\n      <th>A01007</th>\n      <td>We should abolish capital punishment</td>\n      <td>against</td>\n      <td>capital punishment is sometimes the only optio...</td>\n    </tr>\n    <tr>\n      <th>A01008</th>\n      <td>We should ban factory farming</td>\n      <td>against</td>\n      <td>factory farming allows for the production of c...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "             Self-direction: thought  Self-direction: action  Stimulation  \\\nArgument ID                                                                 \nA01002                             0                       0            0   \nA01005                             0                       0            0   \nA01006                             0                       0            0   \nA01007                             0                       0            0   \nA01008                             0                       0            0   \n\n             Hedonism  Achievement  Power: dominance  Power: resources  Face  \\\nArgument ID                                                                    \nA01002              0            0                 0                 0     0   \nA01005              0            0                 0                 0     0   \nA01006              0            0                 1                 0     0   \nA01007              0            0                 0                 0     0   \nA01008              0            0                 0                 0     0   \n\n             Security: personal  Security: societal  Tradition  \\\nArgument ID                                                      \nA01002                        0                   1          0   \nA01005                        1                   0          0   \nA01006                        0                   1          0   \nA01007                        0                   1          0   \nA01008                        1                   0          0   \n\n             Conformity: rules  Conformity: interpersonal  Humility  \\\nArgument ID                                                           \nA01002                       0                          0         0   \nA01005                       0                          0         0   \nA01006                       0                          0         0   \nA01007                       1                          0         0   \nA01008                       0                          0         0   \n\n             Benevolence: caring  Benevolence: dependability  \\\nArgument ID                                                    \nA01002                         0                           0   \nA01005                         0                           0   \nA01006                         0                           0   \nA01007                         0                           0   \nA01008                         1                           0   \n\n             Universalism: concern  Universalism: nature  \\\nArgument ID                                                \nA01002                           0                     0   \nA01005                           0                     0   \nA01006                           0                     0   \nA01007                           1                     0   \nA01008                           1                     0   \n\n             Universalism: tolerance  Universalism: objectivity  \nArgument ID                                                      \nA01002                             0                          0  \nA01005                             0                          0  \nA01006                             0                          0  \nA01007                             0                          0  \nA01008                             0                          0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Self-direction: thought</th>\n      <th>Self-direction: action</th>\n      <th>Stimulation</th>\n      <th>Hedonism</th>\n      <th>Achievement</th>\n      <th>Power: dominance</th>\n      <th>Power: resources</th>\n      <th>Face</th>\n      <th>Security: personal</th>\n      <th>Security: societal</th>\n      <th>Tradition</th>\n      <th>Conformity: rules</th>\n      <th>Conformity: interpersonal</th>\n      <th>Humility</th>\n      <th>Benevolence: caring</th>\n      <th>Benevolence: dependability</th>\n      <th>Universalism: concern</th>\n      <th>Universalism: nature</th>\n      <th>Universalism: tolerance</th>\n      <th>Universalism: objectivity</th>\n    </tr>\n    <tr>\n      <th>Argument ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A01002</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>A01005</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>A01006</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>A01007</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>A01008</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert files in dataframes\n",
    "train_arg_df, val_arg_df, test_arg_df = create_dfs(ARGUMENTS_DIR)\n",
    "train_labels_df, val_labels_df, test_labels_df = create_dfs(LABELS_DIR)\n",
    "\n",
    "print(\"Let's visualize the data: \")\n",
    "display(train_arg_df.head(5))\n",
    "display(train_labels_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8c631fc728aa8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Mapping labels to level-3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998d8c9bd76a90f4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:49:33.911866385Z",
     "start_time": "2023-12-13T11:49:32.962567448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training labels after the mapping are the following: \n"
     ]
    },
    {
     "data": {
      "text/plain": "             Openess_to_change  Self_enhancement  Conservation  \\\nArgument ID                                                      \nA01002                       0                 0             1   \nA01005                       0                 0             1   \nA01006                       0                 1             1   \nA01007                       0                 0             1   \nA01008                       0                 0             1   \n...                        ...               ...           ...   \nE08016                       0                 1             1   \nE08017                       0                 0             1   \nE08018                       0                 0             0   \nE08019                       0                 0             1   \nE08020                       1                 1             1   \n\n             Self_transcendence  \nArgument ID                      \nA01002                        0  \nA01005                        0  \nA01006                        0  \nA01007                        0  \nA01008                        1  \n...                         ...  \nE08016                        1  \nE08017                        1  \nE08018                        1  \nE08019                        1  \nE08020                        1  \n\n[5393 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Openess_to_change</th>\n      <th>Self_enhancement</th>\n      <th>Conservation</th>\n      <th>Self_transcendence</th>\n    </tr>\n    <tr>\n      <th>Argument ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A01002</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>A01005</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>A01006</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>A01007</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>A01008</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>E08016</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>E08017</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>E08018</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>E08019</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>E08020</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5393 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = define_mapping()\n",
    "train_labels_df, val_labels_df, test_labels_df = map_to_level_3(mapping, train_labels_df, val_labels_df, test_labels_df) \n",
    "\n",
    "print(\"The training labels after the mapping are the following: \")\n",
    "train_labels_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3b5162e441c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### One-hot encoding, tokenization and data loaders building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "if os.path.exists(BERT_MODELS_DIRECTORY):\n",
    "    bert_versions_paths = os.listdir(BERT_MODELS_DIRECTORY)\n",
    "else:    \n",
    "    bert_versions_paths = download_bert_models(BERT_MODELS_DIRECTORY, BERT_VERSIONS)  \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:49:33.984198597Z",
     "start_time": "2023-12-13T11:49:33.912027652Z"
    }
   },
   "id": "1dad71e1264e68ca"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e75074e5c73c20",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:49:39.928353375Z",
     "start_time": "2023-12-13T11:49:33.953295504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 38])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1])\n",
      "\n",
      "torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "model_inputs = {\"C\":[\"Conclusion\"], \"CP\":[\"Conclusion\", \"Premise\"], \"CPS\":[\"Conclusion\", \"Premise\", \"Stance\"]}\n",
    "\n",
    "dataloader_train_C, dataloader_val_C, dataloader_test_C  = build_dataloaders(train_df=train_arg_df, \n",
    "                                                                             val_df=val_arg_df, \n",
    "                                                                             test_df=test_arg_df, \n",
    "                                                                             train_labels_df=train_labels_df, \n",
    "                                                                             val_labels_df=val_labels_df, \n",
    "                                                                             test_labels_df=test_labels_df, \n",
    "                                                                             one_hot_mapping=CLASS_2_ONE_HOT, \n",
    "                                                                             bert_version=\"bert-base-uncased\", \n",
    "                                                                             model_input=model_inputs[\"C\"],\n",
    "                                                                             custom_dataset_builder=CustomDataset_C,\n",
    "                                                                             batch_size=BATCH_SIZE, \n",
    "                                                                             shuffle=True)\n",
    "\n",
    "dataloader_train_CP, dataloader_val_CP, dataloader_test_CP  = build_dataloaders(train_df=train_arg_df, \n",
    "                                                                                val_df=val_arg_df, \n",
    "                                                                                test_df=test_arg_df, \n",
    "                                                                                train_labels_df=train_labels_df, \n",
    "                                                                                val_labels_df=val_labels_df, \n",
    "                                                                                test_labels_df=test_labels_df, \n",
    "                                                                                one_hot_mapping=CLASS_2_ONE_HOT, \n",
    "                                                                                bert_version=\"bert-base-uncased\", \n",
    "                                                                                model_input=model_inputs[\"CP\"],\n",
    "                                                                                custom_dataset_builder=CustomDataset_CP,\n",
    "                                                                                batch_size=BATCH_SIZE, \n",
    "                                                                                shuffle=True)\n",
    "\n",
    "dataloader_train_CPS, dataloader_val_CPS, dataloader_test_CPS  = build_dataloaders(train_df=train_arg_df, \n",
    "                                                                                   val_df=val_arg_df, \n",
    "                                                                                   test_df=test_arg_df, \n",
    "                                                                                   train_labels_df=train_labels_df, \n",
    "                                                                                   val_labels_df=val_labels_df, \n",
    "                                                                                   test_labels_df=test_labels_df, \n",
    "                                                                                   one_hot_mapping=CLASS_2_ONE_HOT, \n",
    "                                                                                   bert_version=\"bert-base-uncased\", \n",
    "                                                                                   model_input=model_inputs[\"CPS\"],\n",
    "                                                                                   custom_dataset_builder=CustomDataset_CPS,\n",
    "                                                                                   batch_size=BATCH_SIZE, \n",
    "                                                                                   shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b4d8eb2b9080d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbdf03bd612820",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Models Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3848b4411be166",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  1) Random uniform classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f51d9c86bef3a0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:49:39.942470583Z",
     "start_time": "2023-12-13T11:49:39.926540823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'# Create an instance of the random uniform classifier\\nrandom_classifier = RandomUniformClassifier(N_LABELS)\\n\\n# Make predictions on the test set\\npredicted_labels = random_classifier.predict(test_arg_df)\\n\\n# Accuracy of the Random Classifier\\naccuracy = accuracy_score(test_labels_df, predicted_labels)\\nprint(f\\'Accuracy of the model over all the classes: {accuracy}\\n\\')\\n\\n# average F1 on the different labels singularly taken \\nf1_scores_random_classifier = f1_labels_score(test_labels_df, predicted_labels, n_labels=N_LABELS)\\nf1_avg_random_classifier = np.average(f1_scores_random_classifier)\\n\\nfor idx, f1_score in enumerate(f1_scores_random_classifier):\\n    print(f\" - {train_labels_df.columns[idx]} F1: {f1_score}\")\\n\\nprint(f\\'\\nAverage F1: {f1_avg_random_classifier}\\')'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Create an instance of the random uniform classifier\n",
    "random_classifier = RandomUniformClassifier(N_LABELS)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_labels = random_classifier.predict(test_arg_df)\n",
    "\n",
    "# Accuracy of the Random Classifier\n",
    "accuracy = accuracy_score(test_labels_df, predicted_labels)\n",
    "print(f'Accuracy of the model over all the classes: {accuracy}\\n')\n",
    "\n",
    "# average F1 on the different labels singularly taken \n",
    "f1_scores_random_classifier = f1_labels_score(test_labels_df, predicted_labels, n_labels=N_LABELS)\n",
    "f1_avg_random_classifier = np.average(f1_scores_random_classifier)\n",
    "\n",
    "for idx, f1_score in enumerate(f1_scores_random_classifier):\n",
    "    print(f\" - {train_labels_df.columns[idx]} F1: {f1_score}\")\n",
    "\n",
    "print(f'\\nAverage F1: {f1_avg_random_classifier}')\"\"\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db911208037aa18",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  1) Majority classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4563d2862bce6247",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T11:49:39.983368991Z",
     "start_time": "2023-12-13T11:49:39.940971957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'majority_classifier = MajorityClassifier()\\n\\n# Train the majority classifier (even though in practice, no training is needed)\\nmajority_classifier.fit(train_labels_df)\\n\\n# Make predictions on the test set\\npredicted_labels = majority_classifier.predict(test_labels_df)\\n\\n# Accuracy of the Majority Classifier\\naccuracy = accuracy_score(test_labels_df, predicted_labels)\\nprint(f\\'Accuracy of the model over all the classes: {accuracy}\\n\\')\\n\\n# Average F1 on the different labels singularly taken \\nf1_scores_majority_classifier = f1_labels_score(test_labels_df, predicted_labels, n_labels=N_LABELS)\\nf1_avg_majority_classifier = np.average(f1_scores_majority_classifier)\\n\\nfor idx, f1_score in enumerate(f1_scores_majority_classifier):\\n    print(f\" - {train_labels_df.columns[idx]} F1: {f1_score}\")\\n\\nprint(f\\'\\nAverage F1: {f1_avg_majority_classifier}\\')\\n'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"majority_classifier = MajorityClassifier()\n",
    "\n",
    "# Train the majority classifier (even though in practice, no training is needed)\n",
    "majority_classifier.fit(train_labels_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_labels = majority_classifier.predict(test_labels_df)\n",
    "\n",
    "# Accuracy of the Majority Classifier\n",
    "accuracy = accuracy_score(test_labels_df, predicted_labels)\n",
    "print(f'Accuracy of the model over all the classes: {accuracy}\\n')\n",
    "\n",
    "# Average F1 on the different labels singularly taken \n",
    "f1_scores_majority_classifier = f1_labels_score(test_labels_df, predicted_labels, n_labels=N_LABELS)\n",
    "f1_avg_majority_classifier = np.average(f1_scores_majority_classifier)\n",
    "\n",
    "for idx, f1_score in enumerate(f1_scores_majority_classifier):\n",
    "    print(f\" - {train_labels_df.columns[idx]} F1: {f1_score}\")\n",
    "\n",
    "print(f'\\nAverage F1: {f1_avg_majority_classifier}')\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed615adcd88b156c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####  3) Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "185abecdef2d8b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T12:05:01.145487636Z",
     "start_time": "2023-12-13T12:03:19.921513747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 1/200 Iterations: 2107/5393 - loss: 7.470613479614258 - F1_macro_avg: 0.1255"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m criterion_test \u001B[38;5;241m=\u001B[39m Criterion(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m, loss_function\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mBCEWithLogitsLoss(reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m'\u001B[39m), reduction_function\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39msum)\n\u001B[1;32m      4\u001B[0m bert1 \u001B[38;5;241m=\u001B[39m BertOne(dropout_prob\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, hidden_size\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m768\u001B[39m, bert_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m----> 6\u001B[0m bert1_history \u001B[38;5;241m=\u001B[39m \u001B[43mbert1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataloader_train_C\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataloader_val_C\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mF1_Score_Multi_Labels\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mF1_macro_avg\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mN_LABELS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mN_CLASSES\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                          \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;66;43;03m#aggregate_loss_on_dataset=False,\u001B[39;49;00m\n\u001B[1;32m     12\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;66;43;03m#early_stopper=EarlyStopper(monitor='F1_macro', patience=4, delta=0, mode='max', restore_weights=True),\u001B[39;49;00m\n\u001B[1;32m     13\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03mloss homework 1 con batch size 1\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m[0.4, 0.5, 0.8, ... 0.2] 46 loss\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     20\u001B[0m \n\u001B[1;32m     21\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP_homework_2/drTorch/modules.py:156\u001B[0m, in \u001B[0;36mTrainableModule.fit\u001B[0;34m(self, train_loader, val_loader, criterion, metrics, optimizer, num_epochs, early_stopper, aggregate_loss_on_dataset, verbose)\u001B[0m\n\u001B[1;32m    154\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m    155\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion\u001B[38;5;241m.\u001B[39mreduction_function(loss)\n\u001B[0;32m--> 156\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    158\u001B[0m metrics_value \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/PycharmProjects/NLP_homework_2/venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP_homework_2/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "optimizer_test = OptimizerWrapper(torch.optim.Adam, identifier=f'lr={1e-5}', optimizer_partial_params={'lr': 1e-6})          \n",
    "criterion_test = Criterion('loss', loss_function=torch.nn.BCEWithLogitsLoss(reduction='none'), reduction_function=torch.sum)\n",
    "\n",
    "bert1 = BertOne(dropout_prob=0.3, hidden_size= 768, bert_version='bert-base-uncased').to(device)\n",
    "\n",
    "bert1_history = bert1.fit(train_loader=dataloader_train_C, \n",
    "                          val_loader=dataloader_val_C, \n",
    "                          criterion=criterion_test, \n",
    "                          metrics=[F1_Score_Multi_Labels('F1_macro_avg', num_labels=N_LABELS, num_classes=N_CLASSES)], \n",
    "                          optimizer=optimizer_test,\n",
    "                          #aggregate_loss_on_dataset=False,\n",
    "                          #early_stopper=EarlyStopper(monitor='F1_macro', patience=4, delta=0, mode='max', restore_weights=True),\n",
    "                          num_epochs=200)\n",
    "\n",
    "\"\"\"\n",
    "loss homework 1 con batch size 1\n",
    "[0.4, 0.5, 0.8, ... 0.2] 46 loss\n",
    "\n",
    "loss homework 2 con batch size 1\n",
    "        [[0.4339, 0.4490],\n",
    "        [0.5538, 0.6676],\n",
    "        [0.4004, 0.3562],\n",
    "        [0.6964, 0.6620]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829599ba89a99187",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T18:43:13.675595100Z",
     "start_time": "2023-12-10T18:43:10.210703400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": "'p_d = BertOne()\\noutput1 = p_d(**encoded_input)'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test sul modello\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config=transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
    "text = \"Replace me by any text you'd like pollo.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "output = model(**encoded_input)\n",
    "print(model.config)\n",
    "\"\"\"p_d = BertOne()\n",
    "output1 = p_d(**encoded_input)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn.functional import max_pool2d\n",
    "\n",
    "print(output[0].shape)\n",
    "output_2 = max_pool2d(output[0], kernel_size=(14,1))\n",
    "\n",
    "output_2[:,0,:].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-10T18:42:21.667086600Z"
    }
   },
   "id": "c2bf2f8851186caf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
